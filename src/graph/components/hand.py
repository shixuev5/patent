# src/graph/components/hand.py

import asyncio
import re
import json
from collections import Counter
from typing import List, Dict, Optional
from loguru import logger

from src.search_clients.factory import SearchClientFactory
from src.utils.llm import get_llm_service
from config import settings

from src.utils.reranker import Reranker


class ExecutionHand:
    """
    ç”Ÿäº§çº§æ‰§è¡Œç»„ä»¶ (ExecutionHand)
    
    æ ¸å¿ƒèŒè´£ï¼š
    1. å¼‚æ­¥æ‰§è¡Œæ£€ç´¢ç­–ç•¥ (Async Execution)
    2. åŠ¨æ€æŸ¥è¯¢é™çº§ (Query Relaxation)
    3. å¤æ‚æ—¥æœŸé€»è¾‘æ³¨å…¥ (Date Logic Injection for E/P docs)
    4. ç»“æœæ‰©å±• (Spider Search: Family & Citations)
    5. æ™ºèƒ½åé¦ˆ (IPC Analysis & Keyword Harvesting)
    """

    def __init__(self):
        # è·å–æ£€ç´¢å®¢æˆ·ç«¯ (å‡è®¾åº•å±‚ Client æ˜¯åŒæ­¥çš„ï¼Œæˆ‘ä»¬å°†é€šè¿‡ asyncio.to_thread åŒ…è£…)
        self.client = SearchClientFactory.get_client("zhihuiya")
        self.llm = get_llm_service()
        self.reranker = Reranker()

    async def execute_batch(
        self, 
        strategies: List[Dict], 
        critical_date: str = "",
        rerank_anchor: str = "" # æ–°å¢ï¼šç”¨äº Rerank çš„é”šç‚¹æ–‡æœ¬
    ) -> List[Dict]:
        """
        [Async] æ‰¹é‡å¹¶å‘æ‰§è¡Œæ£€ç´¢ç­–ç•¥ã€‚
        
        Args:
            strategies: ç­–ç•¥åˆ—è¡¨
            critical_date: æŸ¥æ–°æˆªæ­¢æ—¥æœŸ (YYYYMMDD)
            
        Returns:
            List[Dict]: å‘½ä¸­çš„æ–‡æ¡£åˆ—è¡¨ (æœªå»é‡)
        """
        if not strategies:
            return []

        logger.info(f"[Hand] ğŸš€ Async executing batch of {len(strategies)} queries...")
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = []
        for strat in strategies:
            tasks.append(
                self._execute_single_strategy_async(strat, critical_date)
            )
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results_list_of_lists = await asyncio.gather(*tasks)
        
        # æ‰å¹³åŒ–ç»“æœ
        all_docs = []
        # ä½¿ç”¨ set ç®€å•å»é‡ (åŸºäº uid)ï¼Œé˜²æ­¢åŒä¸€æ‰¹æ¬¡å†…é‡å¤
        seen_ids = set()
        
        for res_list in results_list_of_lists:
            for doc in res_list:
                uid = doc.get("uid") or doc.get("id") or doc.get("pn")
                if uid and uid not in seen_ids:
                    all_docs.append(doc)
                    seen_ids.add(uid)
                    
        # 4. [P0-2] è¯­ä¹‰é‡æ’åº (Rerank)
        # åœ¨è¿”å›å‰ï¼Œå¯¹ç»“æœè¿›è¡Œä¸€æ¬¡é‡æ’ï¼Œç¡®ä¿ä¸‹æ¸¸ Reviewer æ‹¿åˆ°çš„æ˜¯æœ€ç›¸å…³çš„
        if rerank_anchor and all_docs:
            logger.info(f"[Hand] âš–ï¸ Reranking {len(all_docs)} docs...")
            all_docs = self.reranker.rank_docs(rerank_anchor, all_docs)
                    
        logger.info(f"[Hand] Batch execution finished. Total unique docs found: {len(all_docs)}")
        return all_docs

    async def _execute_single_strategy_async(
        self, 
        strat: Dict, 
        date: str
    ) -> List[Dict]:
        """
        æ‰§è¡Œå•ä¸ªç­–ç•¥çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸï¼š
        Date Injection -> Search -> (Zero Hit) -> Relaxation -> Retry -> Noise Filter
        """
        q_str = strat.get("query")
        db_type = strat.get("db")
        intent = strat.get("intent", "Broad")

        if not q_str:
            return []

        # 1. æ³¨å…¥æ—¥æœŸé™åˆ¶ (Core Logic)
        final_query = self._inject_date(q_str, db_type, date, intent)
        found_docs = []

        try:
            # --- Attempt 1: Strict Execution ---
            # ä½¿ç”¨ to_thread å°†åŒæ­¥ IO è½¬æ¢ä¸ºå¼‚æ­¥
            logger.debug(f"Running [{intent}]: {final_query[:60]}...")
            
            response = await asyncio.to_thread(
                self.client.search, query=final_query, limit=100
            )
            
            total = response.get("total", 0)
            hits = response.get("results", [])

            # --- Attempt 2: Relaxation (Step 5 Feature) ---
            # å¦‚æœä¸¥æ ¼æ£€ç´¢ä¸º 0ï¼Œä¸”ä¸æ˜¯ E ç±»æ£€ç´¢ (Eç±»é€šå¸¸å°±è¦ä¸¥ï¼Œä¸èƒ½éšä¾¿æ”¾å®½)ï¼Œå°è¯•é™çº§
            if total == 0 and intent != "Conflicting_E":
                relaxed_query = self._relax_query(final_query)
                
                # åªæœ‰å½“ Query çœŸçš„å‘ç”Ÿäº†å˜åŒ–æ‰é‡è¯•
                if relaxed_query and relaxed_query != final_query:
                    logger.info(f"  â†³ Zero hits. Retrying with relaxed: {relaxed_query[:50]}...")
                    strat["status"] = "executed_relaxed"
                    strat["relaxed_query"] = relaxed_query
                    
                    # é™çº§é‡è¯• (é™åˆ¶è¿”å›æ•°é‡ï¼Œé˜²æ­¢å™ªéŸ³)
                    response = await asyncio.to_thread(
                        self.client.search, query=relaxed_query, limit=50
                    )
                    hits = response.get("results", [])
                    total = response.get("total", 0)

            # --- Noise Circuit Breaker ---
            # å¦‚æœç»“æœè¿‡å¤šï¼Œé€šå¸¸æ„å‘³ç€ Query å¤ªå®½æ³›ï¼Œå‚è€ƒä»·å€¼ä½
            max_hits = 2000 if intent == "Broad" else 1000
            if total > max_hits:
                logger.warning(f"Query returned {total} hits (>{max_hits}). Treating as Noise.")
                strat["status"] = "skipped_noise"
                return []

            if total == 0:
                strat["status"] = "executed_empty"
                return []

            if strat.get("status") != "executed_relaxed":
                strat["status"] = "executed_success"

            # --- Result Normalization ---
            for doc in hits:
                # è¡¥å……å…ƒæ•°æ®
                doc["uid"] = doc.get("id") or doc.get("pn")
                doc["source_strategy"] = strat.get("name")
                doc["source_intent"] = intent
                
                # æ ‡è®°æ—¥æœŸé€»è¾‘ç±»å‹ï¼Œè¾…åŠ© Reranker åŠ æƒ
                if intent == "Conflicting_E":
                    doc["check_date_logic"] = "Conflicting"
                else:
                    doc["check_date_logic"] = "PriorArt"
                
                found_docs.append(doc)

        except Exception as e:
            logger.error(f"Query execution failed: {e}")
            strat["status"] = "error"

        return found_docs

    def _inject_date(self, query: str, db: str, date: str, intent: str) -> str:
        """
        [Step 3 Feature] ç»™ Query åŠ ä¸Šæ—¶é—´é™åˆ¶ã€‚
        æ”¯æŒ E ç±» (æŠµè§¦ç”³è¯·) çš„ç‰¹æ®Šé€»è¾‘ã€‚
        """
        if not date:
            return query

        # é€‚é… Patsnap / Zhihuiya è¯­æ³•
        if "Patsnap" in db or "Zhihuiya" in db:
            if intent == "Conflicting_E":
                # Eç±»å®šä¹‰: ç”³è¯·æ—¥ <= æŸ¥æ–°æ—¥ AND å…¬å¼€æ—¥ > æŸ¥æ–°æ—¥
                # æ³¨æ„: å…·ä½“è¾¹ç•Œ(åŒ…å«/ä¸åŒ…å«)éœ€æ ¹æ®æ•°æ®åº“æ–‡æ¡£å¾®è°ƒï¼Œè¿™é‡Œé‡‡ç”¨é€šç”¨é€»è¾‘
                return f"({query}) AND APD:[* TO {date}] AND PBD:[{date} TO *]"
            else:
                # å¸¸è§„/Pç±»å®šä¹‰: å…¬å¼€æ—¥ <= æŸ¥æ–°æ—¥
                return f"({query}) AND PBD:[* TO {date}]"

        return query

    def _relax_query(self, query: str) -> Optional[str]:
        """
        [Step 5 Feature] åŸºäºæ­£åˆ™çš„æŸ¥è¯¢é™çº§é€»è¾‘ã€‚
        Hierachy: s -> p -> AND
        """
        original = query
        relaxed = query

        # 1. 's' (Same Sentence) -> 'p' (Same Paragraph)
        # åŒ¹é…: ) s ( æˆ– )s( (ä¸åŒºåˆ†å¤§å°å†™)
        if re.search(r"\)\s*s\s*\(", relaxed, re.IGNORECASE):
            relaxed = re.sub(r"\)\s*s\s*\(", ") p (", relaxed, flags=re.IGNORECASE)
        
        # 2. 'w/n' (Within Words) -> 'p'
        elif re.search(r"w/\d+", relaxed, re.IGNORECASE):
             relaxed = re.sub(r"w/\d+", "p", relaxed, flags=re.IGNORECASE)

        # 3. 'p' (Same Paragraph) -> 'AND'
        # åªæœ‰å½“å‰å·²ç»æ˜¯ pï¼Œä¸”ä¸Šä¸€æ­¥æ²¡å˜è¿‡ï¼Œæ‰é™çº§åˆ° AND
        elif re.search(r"\)\s*p\s*\(", relaxed, re.IGNORECASE):
            relaxed = re.sub(r"\)\s*p\s*\(", ") AND (", relaxed, flags=re.IGNORECASE)
        
        if relaxed == original:
            return None
            
        return relaxed

    async def expand_high_value_docs(self, docs: List[Dict]) -> List[Dict]:
        """
        [Step 4 Feature] Spider Search (åŒæ—ä¸å¼•è¯æ‰©å±•)
        Async implementation.
        """
        # åªå¯¹ Top 3 è¿›è¡Œæ‰©å±•ï¼Œé˜²æ­¢çˆ†ç‚¸
        seed_docs = docs[:3] 
        if not seed_docs:
            return []

        logger.info(f"[Hand] ğŸ•·ï¸ Running Spider Search on {len(seed_docs)} seed docs...")
        
        async def fetch_lineage(seed_doc):
            uid = seed_doc.get("uid")
            if not uid: return []
            
            lineage_docs = []
            try:
                # å¹¶å‘è¯·æ±‚åŒæ—å’Œå¼•è¯
                # å‡è®¾ client æä¾›äº† get_family å’Œ get_citations æ–¹æ³•
                fams, cits = await asyncio.gather(
                    asyncio.to_thread(self.client.get_family, uid),
                    asyncio.to_thread(self.client.get_citations, uid),
                    return_exceptions=True
                )
                
                if isinstance(fams, list):
                    for d in fams:
                        d["source_strategy"] = "Spider_Family"
                        d["source_intent"] = "Lineage"
                        lineage_docs.append(d)
                
                if isinstance(cits, list):
                    for d in cits:
                        d["source_strategy"] = "Spider_Citation"
                        d["source_intent"] = "Lineage"
                        lineage_docs.append(d)
                        
            except Exception as e:
                logger.warning(f"Spider failed for {uid}: {e}")
            
            return lineage_docs

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ç§å­çš„æ‰©å±•
        results = await asyncio.gather(*[fetch_lineage(d) for d in seed_docs])
        
        expanded_docs = []
        for res in results:
            expanded_docs.extend(res)

        logger.info(f"[Hand] ğŸ•·ï¸ Spider found {len(expanded_docs)} related docs.")
        return expanded_docs

    def analyze_ipcs(self, docs: List[Dict], top_n: int = 8) -> List[str]:
        """
        [Step 6 Feature] IPC Calibration
        ç»Ÿè®¡å‘½ä¸­ç»“æœçš„é«˜é¢‘åˆ†ç±»å· (å¤§ç»„çº§)ã€‚
        """
        if not docs:
            return []
            
        ipc_counter = Counter()
        
        for doc in docs:
            # å…¼å®¹ä¸åŒçš„å­—æ®µå
            ipcs = doc.get("ipc_classifications", []) or doc.get("ipcs", [])
            if isinstance(ipcs, str):
                ipcs = [ipcs]
                
            for ipc in ipcs:
                # æå–å¤§ç»„ (e.g., "H04W 72/04" -> "H04W 72")
                # æ­£åˆ™åŒ¹é…: 4ä½å­ç±» + ç©ºæ ¼ + æ•°å­—
                match = re.match(r"^([A-H]\d{2}[A-Z]\s?\d+)", ipc.strip())
                if match:
                    group = match.group(1).replace(" ", "") # H04W72
                    ipc_counter[group] += 1
        
        if not ipc_counter:
            return []
            
        # ç®€å•çš„é˜ˆå€¼è¿‡æ»¤ï¼šè‡³å°‘å‡ºç° 2 æ¬¡
        valid_ipcs = [
            ipc for ipc, count in ipc_counter.most_common(top_n) 
            if count >= 2
        ]
        
        if valid_ipcs:
            logger.info(f"[Hand] ğŸ“Š Validated IPCs: {valid_ipcs}")
            
        return valid_ipcs

    def harvest_new_keywords(
        self, 
        docs: List[Dict], 
        current_matrix: List[Dict]
    ) -> List[Dict]:
        """
        [Step 8 Feature] Keyword Harvesting
        ä½¿ç”¨ Fast Model ä»é«˜ç›¸å…³æ–‡æ¡£ä¸­å­¦ä¹ æ–°è¯æ±‡ã€‚
        """
        if not docs:
            return current_matrix

        # 1. å‡†å¤‡è¯­æ–™ (å– Top 3)
        candidates = docs[:3]
        corpus_parts = []
        for d in candidates:
            corpus_parts.append(f"Title: {d.get('title')}\nAbs: {d.get('abstract')}")
        corpus_str = "\n---\n".join(corpus_parts)

        # 2. å‡†å¤‡ç°æœ‰æ¦‚å¿µçš„ç®€åŒ–ç‰ˆ (å‡å°‘ Token)
        matrix_lite = []
        for item in current_matrix:
            matrix_lite.append({
                "id": item["concept_key"],
                "zh": item.get("zh_expand", [])[:5], # åªç»™å‰5ä¸ªä½œä¸ºæç¤º
                "en": item.get("en_expand", [])[:5]
            })

        # 3. è°ƒç”¨ Fast Model
        prompt = f"""
        Extract NEW synonyms for the concepts below from the patent text provided.
        
        Concepts (ID: Existing Keywords):
        {json.dumps(matrix_lite, ensure_ascii=False)}

        Patent Text Corpus:
        {corpus_str[:3000]} # Truncate

        Instructions:
        1. Find synonyms/related terms present in the text but NOT in the existing list.
        2. Output JSON list.
        
        Output Format:
        [
            {{"id": "concept_id", "new_zh": ["è¯1"], "new_en": ["term1"]}}
        ]
        """

        try:
            resp = self.llm.chat_completion_json(
                model=settings.LLM_MODEL_FAST, # ä½¿ç”¨å»‰ä»·æ¨¡å‹
                messages=[{"role": "user", "content": prompt}]
            )
            
            if not isinstance(resp, list):
                return current_matrix
            
            # 4. åˆå¹¶æ›´æ–°
            # æ·±æ‹·è´ current_matrix ä»¥é¿å…å‰¯ä½œç”¨
            updated_matrix = [dict(item) for item in current_matrix]
            matrix_map = {item["concept_key"]: item for item in updated_matrix}
            
            has_update = False
            for update in resp:
                cid = update.get("id")
                if cid in matrix_map:
                    # Update ZH
                    new_zh = set(update.get("new_zh", [])) - set(matrix_map[cid].get("zh_expand", []))
                    if new_zh:
                        matrix_map[cid]["zh_expand"].extend(list(new_zh))
                        has_update = True
                    
                    # Update EN
                    new_en = set(update.get("new_en", [])) - set(matrix_map[cid].get("en_expand", []))
                    if new_en:
                        matrix_map[cid]["en_expand"].extend(list(new_en))
                        has_update = True
            
            if has_update:
                logger.info("[Hand] ğŸŒ¾ Harvested new keywords into Matrix.")
                return updated_matrix
                
        except Exception as e:
            logger.warning(f"Keyword harvesting failed: {e}")

        return current_matrix